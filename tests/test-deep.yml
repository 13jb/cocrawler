Crawl:
  MaxHostQPS: 10000  # essentially disables rate limiting
  MaxWorkers: 100
  MaxDepth: 100000
  GlobalBudget: 5000 # prevent a runaway
  UserAgent: cocrawler-test/0.01

Plugins:
  url_allowed: SeedsHostname

Multiprocess:
  ParseInBurnerSize: 1 # make sure the burner thread gets used 100%
  Affinity: yes

Seeds:
  Hosts:
  - http://test.website/ordinary/0
  - http://test.website/ordinary/1 # makes the robots fetch interlock fire
  CrawledHosts:
  - http://test.website/ordinary/3

Logging:
#  LoggingLevel: 3
  Crawllog: crawllog.jsonl
  Frontierlog: frontierlog
  Robotslog: robotslog.jsonl

UserAgent:
  Style: crawler
  MyPrefix: test-deep
  URL: http://example.com/cocrawler.html

Testing:
  TestHostmapAll: 127.0.0.1:8080
#  TestHostmapAll: localhost:8080 ## aiodns doesn't consult /etc/hosts :-/
  StatsEQ:
    fetch URLs: 999
    fetch http code=200: 999
    max urls found on a page: 3
    robots denied: 999  # the url ^/denied/ is on every page
    warc r/r (prefix Testing): 999
    parser in burner thread: 999
    parser in main thread: 0

System:
  RLIMIT_AS_gigabytes: 8
