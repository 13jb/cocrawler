Crawl:
  MaxHostQPS: 10000  # essentially disables rate limiting
  MaxWorkers: 100
  MaxDepth: 100000
  MaxCrawledUrls: 5000 # prevent a runaway
  UserAgent: cocrawler-test/0.01

Plugins:
  url_allowed: SeedsHostname

Multiprocess:
  ParseInBurnerSize: 1 # make sure the burner thread gets used 100%
  Affinity: yes

Seeds:
  Hosts:
  - http://test.website/ordinary/0
  - http://test.website/ordinary/1 # makes the robots fetch interlock fire

Logging:
#  LoggingLevel: 3
  Crawllog: crawllog.jsonl
  Robotslog: robotslog.jsonl

UserAgent:
  Style: crawler
  MyPrefix: test-deep
  URL: http://example.com/cocrawler.html

Testing:
  TestHostmapAll: 127.0.0.1:8080
#  TestHostmapAll: localhost:8080 ## aiodns doesn't consult /etc/hosts :-/
  StatsEQ:
    fetch URLs: 1000
    fetch http code=200: 1000
    max urls found on a page: 3
#    robots denied: 1
    robots denied: 4  # one url, with retries
    warc r/r (prefix Testing): 1000
    parse in burner thread: 1000
    parse in main thread: 0

System:
  RLIMIT_AS_gigabytes: 8
