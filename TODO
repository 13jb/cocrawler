Short-term TODO list:

hook up to AccumulateEvent for politeness
do something with t_last_byte?
compute cpu-boundedness? do we believe our own timers?


AccumulateTime for qps graphs

wire in warc code -- pip warc is BSD
  doc for package does not explain how to generate good headers
  use example WARC generated by Heritrix
   http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf
   http://www.netpreserve.org/sites/default/files/resources/WARC_Guidelines_v1.pdf
    recommends: Prefix-Timestamp-Serial-Crawlhost.war.gz
     prefix: project or crawl name
     timetamp: file creation time
     serial: unique wrt prefix
     crawlhost: domain name or ip
    recommends: WARC-Record-ID be globally unique
     UUID has a standard http://www.ietf.org/rfc/rfc4122.txt
    recommends: warcinfo at start of each file ('./header' in example)
     yes do have redundant info here
     stick the original filename as WARC-Filename just in case file has been renamed
     have a WARC-Warcinfo-ID field for every record (note: Heretrix does not do this)
    recommended: at crawl end
     stuff final logfiles etc in a new warc as resource records
     yeah, may have to segment this warc file
     create a metadata record that has a manifest of all warcs created in the crawl


status REST calls
 hook up to dashboard like https://github.com/christabor/flask_jsondash
qps of 200 text/html + last 10
qps of dns lookups + last 10
qps of raw robots.txt lookups + last 10
coroutine counts
copy of configuration files
slow burners
slow domains
dump more of this at exit or save

find a html torture test? hm not so googlable

ability to add seeds while crawler is running?
 also alter anything in config

steal cosrlib/document/html/htmlencoding.py to be a better
  guesser of page encoding - it works on common crawl ...
  compare with the thing in aiohttp.response.text, which just
   uses cchardet
  What happens when pages lie about their encoding? cosr believes it.
  in many cases I'd rather ignore the encoding 

plugin system revamp

add css parsing and parse embeds to a mocked test

Polite crawling
 rate affected by: 5xx esp 503s, slow replies, and crawl-delay in robots
 increase rate until bad things happen
 remember enough past state so that we can learn something like an
  nginx hard rate limit per IP, but not so much that a bad few minutes
  causes us to stay slow for days
   remember high water marks, so for example, "we did 4-parallel successfully
   for a 5 minute period"
 4 parallel connections is a browser default but that includes embeds
 use site size to affect rate

cpu bound or not
 time.time() vs. time.clock() to mark cpu bound -- perhaps its own coroutine?
 while cpu bound, we can't really trust the slow reply detector

get an example of each usecase written up

release version 1.0 ----------------------------------------

investigate bloom filters (or cuckoo filters) to speed up seen etc.
 e.g. in-site link filter might be something that's almost always usable

re2 for regex parsing speed? difficult to install.

write Apache-licensed surt module -- what should the default policy be?!?!

parser-based extraction of links and embeds - see commonsearch gumbocy

Per-host queues; domain, publicprefix, cctld stats

Datalayer-using queue-fillers

helpers to fetch from IA cdx: domain count, detailed reports, guesses at irrelevant cgi args

seed from sitemap
helpers to fetch from Alexa million, quantcast million


