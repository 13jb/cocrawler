Short-term TODO list:

hook up to AccumulateEvent for politeness - 5xx
  do something real with t_{first,last}_byte? - slow replies as failures
    alternately monitor simultaneous connections? slower => wider => hits limit
    do need to learn that amazon replies slowly but can go super-wide
  compute cpu-boundedness?
    do we believe our own timers?
    did we screw up trying to farm out burn to other processes?

Multiprocessing for burners like html parsing.
  Cheapest regex is 20ms/mb -- or 50kbytes in 1ms, or 50 MB/s
  published example: 2 nodes, 2 billion pages in 3 months; 256qps = 200 megabits, pagesize=100kb
  I'd like to crawl @ 1 gigbit on a small number of servers, 1 core driving the bus each.

url-add injects homepage of domain and soft404 detection - test on top 1k sites
 ride-along post-fetch soft404 processing
 build database of soft404 phrases 

Space Saving Heavy Hitters
  http://www.cse.ust.hk/~raywong/comp5331/References/EfficientComputationOfFrequentAndTop-kElementsInDataStreams.pdf
  nice because it estimates the error! on read-back, can drop poorly-estimated values
  HH also called "top k"
Hierarchical Heavy Hitters to figure out hot paths in every host
  https://arxiv.org/pdf/1102.5540v2.pdf

AccumulateTime for qps graphs

wire in warc code -- pip warc is BSD
  doc for package does not explain how to generate good headers
  use example WARC generated by Heritrix
   http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf
   http://www.netpreserve.org/sites/default/files/resources/WARC_Guidelines_v1.pdf
    recommends: Prefix-Timestamp-Serial-Crawlhost.war.gz
     prefix: project or crawl name
     timetamp: file creation time
     serial: unique wrt prefix
     crawlhost: domain name or ip
    recommends: WARC-Record-ID be globally unique
     UUID has a standard http://www.ietf.org/rfc/rfc4122.txt
    recommends: warcinfo at start of each file ('./header' in example)
     yes do have redundant info here
     stick the original filename as WARC-Filename just in case file has been renamed
     have a WARC-Warcinfo-ID field for every record (note: Heretrix does not do this)
    recommended: at crawl end
     stuff final logfiles etc in a new warc as resource records
     yeah, may have to segment this warc file if logs are large
     create a metadata record that has a manifest of all warcs created in the crawl

status REST calls
 hook up to dashboard like https://github.com/christabor/flask_jsondash
 qps of 200 text/html + last 10
 qps of dns lookups + last 10
 qps of raw robots.txt lookups + last 10
 coroutine counts
 copy of configuration files
 slow burners
 slow domains
 dump more of this at exit or save

find a html torture test? hm not so googlable

ability to add seeds while crawler is running?
 also alter anything in config

steal cosrlib/document/html/htmlencoding.py to be a better
  guesser of page encoding - it works on common crawl ...
  compare with the thing in aiohttp.response.text, which just
   uses cchardet
  What happens when pages lie about their encoding? cosr believes it.
  in many cases I'd rather ignore the encoding 

plugin system revamp

add css parsing and parse embeds to a mocked test

Polite crawling
 rate affected by: 5xx esp 503s, slow replies, and crawl-delay in robots
 increase rate until bad things happen
 remember enough past state so that we can learn something like an
  nginx hard rate limit per IP, but not so much that a bad few minutes
  causes us to stay slow for days
   remember high water marks, so for example, "we did 4-parallel successfully
   for a 5 minute period"
 4 parallel connections is a browser default but that includes embeds
 use site size to affect rate

cpu bound or not
 time.time() vs. time.clock() to mark cpu bound -- perhaps its own coroutine?
 while cpu bound, we can't really trust the slow reply detector

get an example of each usecase written up

release version 1.0 ----------------------------------------

chck out uvloop / httptools -- seems like uvloop+aiohttp might not
have any speedup for me? Although discussion on aiohttp github says
that it's unlikely to be much of a speedup for other than microbenchmarks.

investigate bloom filters (or cuckoo filters) to speed up seen etc.
 e.g. in-site link filter might be something that's almost always usable

re2 for regex parsing speed? difficult to install.

write Apache-licensed surt module -- what should the default policy be?!?!

parser-based extraction of links and embeds - see commonsearch gumbocy

Per-host queues; domain, publicprefix, cctld stats

Datalayer-using queue-fillers

helpers to fetch from IA cdx: domain count, detailed reports, guesses at irrelevant cgi args

seed from sitemap
helpers to fetch from Alexa million, quantcast million


