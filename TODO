Short-term TODO list:

status REST calls
 hook up to dashboard like https://github.com/christabor/flask_jsondash
qps of 200 text/html + last 10
qps of dns lookups + last 10
qps of raw robots.txt lookups + last 10
coroutine counts
copy of configuration files
slow burners
slow domains
dump more of this at exit or save

find a html torture test? hm not so googlable

ability to add seeds while crawler is running?
 also alter anything in config

steal cosrlib/document/html/htmlencoding.py to be a better
guesser of page encoding - it works on common crawl ...
compare with the thing in aiohttp.response.text, which just
uses cchardet
 What happens when pages lie about their encoding? cosr believes it.

PriorityQueue ! (prio, work), low prio goes first
 prio = pathlength to seed ... eventually combined with rank?
 add in a random number between 0-1 to shake things up a bit
  right now the queue advances in order due to the ridealong id being a linear increment
  random ID means that we won't crawl a set of discovered linkes all together
  don't randomize embeds, we want them to go in clusters.

plugin system revamp

add css parsing and parse embeds to a mocked test

Polite crawling
 rate affected by: 5xx esp 503s, slow replies, and crawl-delay in robots
 increase rate until bad things happen
 remember enough past state so that we can learn something like an
  nginx hard rate limit per IP, but not so much that a bad few minutes
  causes us to stay slow for days
   remember high water marks, so for example, "we did 4-parallel successfully
   for a 5 minute period"
 4 parallel connections is a browser default but that includes embeds
 use site size to affect rate

cpu bound or not
 time.time() vs. time.clock() to mark cpu bound -- perhaps its own coroutine?
 while cpu bound, we can't really trust the slow reply detector

Save/Restore queues -- perhaps reranking the priorities?
 shorter path-to-seed might have been found, plus rank could update

wire in warc code -- pip warc is BSD
  doc for package does not explain how to generate good headers
  use example WARC generated by Heritrix

get an example of each usecase written up

release version 1.0 ----------------------------------------

investigate bloom filters (or cuckoo filters) to speed up seen etc.
 e.g. in-site link filter might be something that's almost always usable

re2 for regex parsing speed? difficult to install.

write Apache-licensed surt module -- what should the default policy be?!?!

parser-based extraction of links and embeds - see commonsearch gumbocy

Per-host queues; domain, publicprefix, cctld stats

Datalayer-using queue-fillers

helpers to fetch from IA cdx: domain count, detailed reports, guesses at irrelevant cgi args

seed from sitemap
helpers to fetch from Alexa million, quantcast million


